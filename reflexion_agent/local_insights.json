{
    "task_description": "Teach the student based on the provide material.",
    "material": "1 | D T\nECISION REES\nThewordsprintedhereareconcepts. LearningObjectives:\nYoumustgothroughtheexperiences. \u2013CarlFrederick \u2022 Explainthedifferencebetween\nmemorizationandgeneralization.\n\u2022 Implementadecisiontreeclassifier.\n\u2022 Takeaconcretetaskandcastitasa\nlearningproblem,withaformalno-\ntionofinputspace,features,output\nspace,generatingdistributionand\nlossfunction.\nAt a basic level, machinelearningisaboutpredictingthefuture\nbasedonthepast. Forinstance,youmightwishtopredicthowmuch\nauserAlicewilllikeamoviethatshehasn\u2019tseen,basedonherrat-\ningsofmoviesthatshehasseen. Thispredictioncouldbebasedon\nmanyfactorsofthemovies: theircategory(drama,documentary,\netc.),thelanguage,thedirectorandactors,theproductioncompany,\netc. Ingeneral,thismeansmakinginformedguessesaboutsomeun-\nobservedpropertyofsomeobject,basedonobservedpropertiesof\nthatobject.\nThefirstquestionwe\u2019llaskis: whatdoesitmeantolearn? In\nordertodeveloplearningmachines,wemustknowwhatlearning\nactuallymeans,andhowtodeterminesuccess(orfailure). You\u2019llsee\nDependencies:None.\nthisquestionansweredinaverylimitedlearningsetting,whichwill\nbeprogressivelyloosenedandadaptedthroughouttherestofthis\nbook. Forconcreteness,ourfocuswillbeonaverysimplemodelof\nlearningcalledadecisiontree.\n1.1 What Does it Mean to Learn?\nAlicehasjustbeguntakingacourseonmachinelearning. Sheknows\nthatattheendofthecourse,shewillbeexpectedtohave\u201clearned\u201d\nallaboutthistopic. Acommonwayofgaugingwhetherornotshe\nhaslearnedisforherteacher,Bob,togiveheraexam. Shehasdone\nwellatlearningifshedoeswellontheexam.\nButwhatmakesareasonableexam? IfBobspendstheentire\nsemestertalkingaboutmachinelearning,andthengivesAlicean\nexamonHistoryofPottery,thenAlice\u2019sperformanceonthisexam\nwillnotberepresentativeofherlearning. Ontheotherhand,ifthe\nexamonlyasksquestionsthatBobhasansweredexactlyduringlec-\ntures,thenthisisalsoabadtestofAlice\u2019slearning,especiallyifit\u2019s\nan\u201copennotes\u201dexam. WhatisdesiredisthatAliceobservesspecific\nexamplesfromthecourse,andthenhastoanswernew,butrelated\nquestionsontheexam. ThistestswhetherAlicehastheabilityto\ndecision trees 9\ngeneralize. Generalizationisperhapsthemostcentralconceptin\nmachinelearning.\nAsaconcreteexample,consideracourserecommendationsystem\nforundergraduatecomputersciencestudents. Wehaveacollection\nofstudentsandacollectionofcourses. Eachstudenthastaken,and\nevaluated,asubsetofthecourses. Theevaluationissimplyascore\nfrom \u22122(terrible)to +2(awesome). Thejoboftherecommender\nsystemistopredicthowmuchaparticularstudent(say,Alice)will\nlikeaparticularcourse(say,Algorithms).\nGivenhistoricaldatafromcourseratings(i.e.,thepast)weare\ntryingtopredictunseenratings(i.e.,thefuture). Now,wecould\nbeunfairtothissystemaswell. WecouldaskitwhetherAliceis\nlikelytoenjoytheHistoryofPotterycourse. Thisisunfairbecause\nthesystemhasnoideawhatHistoryofPotteryevenis,andhasno\npriorexperiencewiththiscourse. Ontheotherhand,wecouldask\nithowmuchAlicewilllikeArtificialIntelligence,whichshetook\nlastyearandratedas +2(awesome). Wewouldexpectthesystemto\npredictthatshewouldreallylikeit,butthisisn\u2019tdemonstratingthat\nthesystemhaslearned: it\u2019ssimplyrecallingitspastexperience. In\ntheformercase,we\u2019reexpectingthesystemtogeneralizebeyondits\nexperience,whichisunfair. Inthelattercase,we\u2019renotexpectingit\ntogeneralizeatall.\nThisgeneralsetupofpredictingthefuturebasedonthepastis\natthecoreofmostmachinelearning. Theobjectsthatouralgorithm\nwillmakepredictionsaboutareexamples. Intherecommendersys-\ntemsetting,anexamplewouldbesomeparticularStudent/Course\npair(suchasAlice/Algorithms). Thedesiredpredictionwouldbethe\nratingthatAlicewouldgivetoAlgorithms.\nslebal\nnwonk\ntraining\nlearning\ndata algorithm\n? test f\nexample\nTomakethisconcrete,Figure1.1showsthegeneralframeworkof\npredicted\ninduction. Wearegiventrainingdataonwhichouralgorithmisex- label\npectedtolearn. ThistrainingdataistheexamplesthatAliceobserves\nFigure1.1:Thegeneralsupervisedap-\ninhermachinelearningcourse,orthehistoricalratingsdatafor\nproachtomachinelearning:alearning\ntherecommendersystem. Basedonthistrainingdata,ourlearning algorithmreadsintrainingdataand\nalgorithminducesafunction f thatwillmapanewexampletoacor- computesalearnedfunction f.This\nfunctioncanthenautomaticallylabel\nrespondingprediction. Forexample,ourfunctionmightguessthat\nfuturetextexamples.\nf(Alice/MachineLearning) mightbehighbecauseourtrainingdata\nsaidthatAlicelikedArtificialIntelligence. Wewantouralgorithm\ntobeabletomakelotsofpredictions,sowerefertothecollection\nofexamplesonwhichwewillevaluateouralgorithmasthetestset.\nThetestsetisacloselyguardedsecret: itisthefinalexamonwhich\nourlearningalgorithmisbeingtested. Ifouralgorithmgetstopeek\natitaheadoftime,it\u2019sgoingtocheatanddobetterthanitshould.\nWhyisitbadifthelearningalgo-\n?\nThegoalofinductivemachinelearningistotakesometraining rithmgetstopeekatthetestdata?\ndataanduseittoinduceafunction f. Thisfunction f willbeevalu-\n10 a course in machine learning\natedonthetestdata. Themachinelearningalgorithmhassucceeded\nifitsperformanceonthetestdataishigh.\n1.2 Some Canonical Learning Problems\nTherearealargenumberoftypicalinductivelearningproblems.\nTheprimarydifferencebetweenthemisinwhattypeofthingthey\u2019re\ntryingtopredict. Herearesomeexamples:\nRegression: tryingtopredictarealvalue. Forinstance,predictthe\nvalueofastocktomorrowgivenitspastperformance. Orpredict\nAlice\u2019sscoreonthemachinelearningfinalexambasedonher\nhomeworkscores.\nBinaryClassification: tryingtopredictasimpleyes/noresponse.\nForinstance,predictwhetherAlicewillenjoyacourseornot.\nOrpredictwhetherauserreviewofthenewestAppleproductis\npositiveornegativeabouttheproduct.\nMulticlassClassification: tryingtoputanexampleintooneofanum-\nberofclasses. Forinstance,predictwhetheranewsstoryisabout\nentertainment,sports,politics,religion,etc. Orpredictwhethera\nCScourseisSystems,Theory,AIorOther.\nRanking: tryingtoputasetofobjectsinorderofrelevance. Forin-\nstance,predictingwhatordertoputwebpagesin,inresponsetoa\nuserquery. OrpredictAlice\u2019srankedpreferencesovercoursesshe\nhasn\u2019ttaken.\nForeachofthesetypesofcanon-\nThereasonthatitisconvenienttobreakmachinelearningprob- icalmachinelearningproblems,\n?\ncomeupwithoneortwoconcrete\nlemsdownbythetypeofobjectthatthey\u2019retryingtopredicthasto\nexamples.\ndowithmeasuringerror. Recallthatourgoalistobuildasystem\nthatcanmake\u201cgoodpredictions.\u201d Thisbegsthequestion: whatdoes\nitmeanforapredictiontobe\u201cgood?\u201d Thedifferenttypesoflearning\nproblemsdifferinhowtheydefinegoodness. Forinstance,inregres-\nsion,predictingastockpricethatisoffby $0.05isperhapsmuch\nbetterthanbeingoffby $200.00. Thesamedoesnotholdofmulti-\nclassclassification. There,accidentallypredicting\u201centertainment\u201d\ninsteadof\u201csports\u201disnobetterorworsethanpredicting\u201cpolitics.\u201d\n1.3 The Decision Tree Model of Learning\nThedecisiontreeisaclassicandnaturalmodeloflearning. Itis\ncloselyrelatedtothefundamentalcomputersciencenotionof\u201cdi-\nvideandconquer.\u201d Althoughdecisiontreescanbeappliedtomany\ndecision trees 11\nisSystems?\nno yes\nlearningproblems,wewillbeginwiththesimplestcase: binaryclas-\nsification.\nlike takenOtherSys?\nSupposethatyourgoalistopredictwhethersomeunknownuser\nno yes\nwillenjoysomeunknowncourse. Youmustsimplyanswer\u201cyes\u201dor\n\u201cno.\u201d Inordertomakeaguess,you\u2019reallowedtoaskbinaryques-\ntionsabouttheuser/courseunderconsideration. Forexample: morning? likedOtherSys?\nYou: IsthecourseunderconsiderationinSystems? no yes no yes\nMe: Yes\nYou: HasthisstudenttakenanyotherSystemscourses? like nah nah like\nMe: Yes\nYou: HasthisstudentlikedmostpreviousSystemscourses? Figure1.2:Adecisiontreeforacourse\nrecommendersystem,fromwhichthe\nMe: No\nin-text\u201cdialog\u201disdrawn.\nYou: Ipredictthisstudentwillnotlikethiscourse.\nThegoalinlearningistofigureoutwhatquestionstoask,inwhat\nordertoaskthem,andwhatanswertopredictonceyouhaveasked\nenoughquestions.\nThedecisiontreeisso-calledbecausewecanwriteoursetofques-\ntionsandguessesinatreeformat,suchasthatinFigure1.2. Inthis\nfigure,thequestionsarewrittenintheinternaltreenodes(rectangles)\n60% like\nandtheguessesarewrittenintheleaves(ovals). Eachnon-terminal\noverall: 40% nah\nnodehastwochildren: theleftchildspecifieswhattodoifthean-\nswertothequestionis\u201cno\u201dandtherightchildspecifieswhattodoif\nitis\u201cyes.\u201d 60%\nInordertolearn,Iwillgiveyoutrainingdata. Thisdataconsists easy:yes 40%\nofasetofuser/courseexamples,pairedwiththecorrectanswerfor\nn 60%\no\ntheseexamples(didthegivenuserenjoythegivencourse?). From 40%\nthis,youmustconstructyourquestions. Forconcreteness,thereisa\nsmalldatasetinTable1intheAppendixofthisbook. Thistraining 82%\ndataconsistsof20courseratingexamples,withcourseratingsand AI:yes 18%\nanswerstoquestionsthatyoumightaskaboutthispair. Wewill n 33%\no\ninterpretratingsof0, +1and +2as\u201cliked\u201dandratingsof \u22122and \u22121 67%\nas\u201chated.\u201d\nInwhatfollows,wewillrefertothequestionsthatyoucanaskas 20%\nfeaturesandtheresponsestothesequestionsasfeaturevalues. The\nsystems:yes 80%\nratingiscalledthelabel. Anexampleisjustasetoffeaturevalues. n o 100%\nAndourtrainingdataisasetofexamples,pairedwithlabels. 0%\nTherearealotoflogicallypossibletreesthatyoucouldbuild,\n80%\nevenoverjustthissmallnumberoffeatures(thenumberisinthe\nmillions). Itiscomputationallyinfeasibletoconsideralloftheseto\ntheory:yes 20%\ntrytochoosethe\u201cbest\u201done. Instead,wewillbuildourdecisiontree n o 40%\n60%\ngreedily. Wewillbeginbyasking:\nIfIcouldonlyaskonequestion,whatquestionwouldIask?\nFigure1.3:Ahistogramoflabelsfor(a)\nYouwanttofindafeaturethatismostusefulinhelpingyouguess\ntheentiredataset;(b-e)theexamples\nwhetherthisstudentwillenjoythiscourse. Ausefulwaytothink inthedatasetforeachvalueofthefirst\nfourfeatures.\n12 a course in machine learning\n1\naboutthisistolookatthehistogramoflabelsforeachfeature. 1Acolleaguerelatedthestoryof\nThisisshownforthefirstfourfeaturesinFigure1.3. Eachhistogram gettinghis8-yearoldnephewto\nguessanumberbetween1and100.\nshowsthefrequencyof\u201clike\u201d/\u201chate\u201dlabelsforeachpossiblevalue\nHisnephew\u2019sfirstfourquestions\nofanassociatedfeature. Fromthisfigure,youcanseethatasking were:Isitbiggerthan20?(YES)Is\niteven?(YES)Doesithavea7init?\nthefirstfeatureisnotuseful: ifthevalueis\u201cno\u201dthenit\u2019shardto\n(NO)Isit80?(NO).Ittook20more\nguessthelabel;similarlyiftheansweris\u201cyes.\u201d Ontheotherhand, questionstogetit,eventhough10\naskingthesecondfeatureisuseful: ifthevalueis\u201cno,\u201dyoucanbe\nshouldhavebeensufficient.At8,\nthenephewhadn\u2019tquitefiguredout\nprettyconfidentthatthisstudentwillhatethiscourse;iftheanswer\nhowtodivideandconquer.http:\nis\u201cyes,\u201dyoucanbeprettyconfidentthatthisstudentwilllikethis //blog.computationalcomplexity.\norg/2007/04/\ncourse.\ngetting-8-year-old-interested-in.\nMoreformally,youwillconsidereachfeatureinturn. Youmight html.\nconsiderthefeature\u201cIsthisaSystem\u2019scourse?\u201d Thisfeaturehastwo\npossiblevalue: noandyes. Someofthetrainingexampleshavean\nanswerof\u201cno\u201d\u2013let\u2019scallthatthe\u201cNO\u201dset. Someofthetraining\nexampleshaveananswerof\u201cyes\u201d\u2013let\u2019scallthatthe\u201cYES\u201dset. For\neachset(NOandYES)wewillbuildahistogramoverthelabels.\nThisisthesecondhistograminFigure1.3. Now,supposeyouwere\ntoaskthisquestiononarandomexampleandobserveavalueof\n\u201cno.\u201d Furthersupposethatyoumustimmediatelyguessthelabelfor\nthisexample. Youwillguess\u201clike,\u201dbecausethat\u2019sthemorepreva-\nlentlabelintheNOset(actually,it\u2019stheonlylabelintheNOset).\nAlternatively,ifyoureceiveananswerof\u201cyes,\u201dyouwillguess\u201chate\u201d\nbecausethatismoreprevalentintheYESset.\nSo,forthissinglefeature,youknowwhatyouwouldguessifyou\nhadto. Nowyoucanaskyourself: ifImadethatguessonthetrain-\ningdata,howwellwouldIhavedone? Inparticular,howmanyex-\nampleswouldIclassifycorrectly? IntheNOset(whereyouguessed\n\u201clike\u201d)youwouldclassifyall10ofthemcorrectly. IntheYESset\n(whereyouguessed\u201chate\u201d)youwouldclassify8(outof10)ofthem\ncorrectly. Sooverallyouwouldclassify18(outof20)correctly. Thus,\nwe\u2019llsaythatthescoreofthe\u201cIsthisaSystem\u2019scourse?\u201d questionis\n18/20.\nHowmanytrainingexamples\nYouwillthenrepeatthiscomputationforeachoftheavailable wouldyouclassifycorrectlyfor\n?\neachoftheotherthreefeatures\nfeaturestous,computethescoresforeachofthem. Whenyoumust\nfromFigure1.3?\nchoosewhichfeatureconsiderfirst,youwillwanttochoosetheone\nwiththehighestscore.\nButthisonlyletsyouchoosethefirstfeaturetoaskabout. This\nisthefeaturethatgoesattherootofthedecisiontree. Howdowe\nchoosesubsequentfeatures? Thisiswherethenotionofdivideand\nconquercomesin. You\u2019vealreadydecidedonyourfirstfeature: \u201cIs\nthisaSystemscourse?\u201d Youcannowpartitionthedataintotwoparts:\ntheNOpartandtheYESpart. TheNOpartisthesubsetofthedata\nonwhichvalueforthisfeatureis\u201cno\u201d;theYEShalfistherest. This\nisthedividestep.\ndecision trees 13\nAlgorithm1 DecisionTreeTrain(data,remainingfeatures)\n1: guess\u2190 mostfrequentanswerindata //defaultanswerforthisdata\n2: ifthelabelsindataareunambiguousthen\n3: return Leaf(guess) //basecase:noneedtosplitfurther\n4: elseifremainingfeaturesisemptythen\n5: return Leaf(guess) //basecase:cannotsplitfurther\n6: else //weneedtoquerymorefeatures\n7: forallf \u2208remainingfeaturesdo\n8: NO\u2190 thesubsetofdataonwhichf=no\n9: YES\u2190 thesubsetofdataonwhichf=yes\n10: score[f]\u2190#ofmajorityvoteanswersinNO\n11: +#ofmajorityvoteanswersinYES\n//theaccuracywewouldgetifweonlyqueriedonf\n12: endfor\n13: f \u2190 thefeaturewithmaximalscore(f)\n14: NO\u2190 thesubsetofdataonwhichf=no\n15: YES\u2190 thesubsetofdataonwhichf=yes\n16: left\u2190 DecisionTreeTrain(NO,remainingfeatures\\{f})\n17: right\u2190 DecisionTreeTrain(YES,remainingfeatures\\{f})\n18: return Node(f,left,right)\n19: endif\nAlgorithm2 DecisionTreeTest(tree,testpoint)\n1:\niftreeisoftheformLeaf(guess)then\n2: return guess\n3:\nelseiftreeisoftheformNode(f,left,right)then\n4: iff =nointestpointthen\n5: return DecisionTreeTest(left,testpoint)\n6: else\n7: return DecisionTreeTest(right,testpoint)\n8: endif\n9: endif\nTheconquerstepistorecurse,andrunthesameroutine(choosing\nthefeaturewiththehighestscore)ontheNOset(togetthelefthalf\nofthetree)andthenseparatelyontheYESset(togettherighthalfof\nthetree).\nAtsomepointitwillbecomeuselesstoqueryonadditionalfea-\ntures. Forinstance,onceyouknowthatthisisaSystemscourse,\nyouknowthateveryonewillhateit. Soyoucanimmediatelypredict\n\u201chate\u201dwithoutaskinganyadditionalquestions. Similarly,atsome\npointyoumighthavealreadyqueriedeveryavailablefeatureandstill\nnotwhittleddowntoasingleanswer. Inbothcases,youwillneedto\ncreatealeafnodeandguessthemostprevalentanswerinthecurrent\npieceofthetrainingdatathatyouarelookingat.\nPuttingthisalltogether,wearriveatthealgorithmshowninAl-\ngorithm1.3.2 Thisfunction, DecisionTreeTrain takestwoargu- 2Therearemorenuancedalgorithms\nforbuildingdecisiontrees,someof\nwhicharediscussedinlaterchaptersof\nthisbook.Theyprimarilydifferinhow\ntheycomputethescorefunction.\n14 a course in machine learning\nments: ourdata,andthesetofas-yetunusedfeatures. Ithastwo\nbasecases: eitherthedataisunambiguous,ortherearenoremaining\nfeatures. Ineithercase,itreturnsa Leaf nodecontainingthemost\nlikelyguessatthispoint. Otherwise,itloopsoverallremainingfea-\nturestofindtheonewiththehighestscore. Itthenpartitionsthedata\nintoaNO/YESsplitbasedonthebestfeature. Itconstructsitsleft\nandrightsubtreesbyrecursingonitself. Ineachrecursivecall,ituses\noneofthepartitionsofthedata,andremovesthejust-selectedfeature\nfromconsideration.\n?\nIsAlgorithm1.3guaranteedto\nThecorrespondingpredictionalgorithmisshowninAlgorithm1.3. terminate?\nThisfunctionrecursesdownthedecisiontree,followingtheedges\nspecifiedbythefeaturevaluesinsometestpoint. Whenitreachesa\nleaf,itreturnstheguessassociatedwiththatleaf.\n1.4 Formalizing the Learning Problem\nAsyou\u2019veseen,thereareseveralissuesthatwemusttakeintoac-\ncountwhenformalizingthenotionoflearning.\n\u2022 Theperformanceofthelearningalgorithmshouldbemeasuredon\nunseen\u201ctest\u201ddata.\n\u2022 Thewayinwhichwemeasureperformanceshoulddependonthe\nproblemwearetryingtosolve.\n\u2022 Thereshouldbeastrongrelationshipbetweenthedatathatour\nalgorithmseesattrainingtimeandthedataitseesattesttime.\nInordertoaccomplishthis,let\u2019sassumethatsomeonegivesusa\nlossfunction, (cid:96)(\u00b7,\u00b7),oftwoarguments. Thejobof (cid:96) istotellushow\n\u201cbad\u201dasystem\u2019spredictionisincomparisontothetruth. Inparticu-\nlar,if y isthetruthand y\u02c6 isthesystem\u2019sprediction,then (cid:96)(y,y\u02c6) isa\nmeasureoferror.\nForthreeofthecanonicaltasksdiscussedabove,wemightusethe\nfollowinglossfunctions:\nRegression: squaredloss (cid:96)(y,y\u02c6) = (y\u2212y\u02c6)2\norabsoluteloss (cid:96)(y,y\u02c6) = |y\u2212y\u02c6|.\n(cid:40)\n0 if y = y\u02c6\nBinaryClassification: zero/oneloss (cid:96)(y,y\u02c6) = Thisnotationmeansthatthelossiszero\n1 otherwise ifthepredictioniscorrectandisone\notherwise.\nMulticlassClassification: alsozero/oneloss.\nWhymightitbeabadideatouse\nNotethatthelossfunctionissomethingthatyoumustdecideon ? zero/onelosstomeasureperfor-\nbasedonthegoalsoflearning. manceforaregressionproblem?\nNowthatwehavedefinedourlossfunction,weneedtoconsider\nwherethedata(trainingandtest)comesfrom. Themodelthatwe\ndecision trees 15\nMATH REVIEW | EXPECTATED VALUES\nWewriteE [(cid:96)(y, f(x))] fortheexpectedloss. Expectationmeans\u201caverage.\u201d Thisissaying\u201cifyou\n(x,y)\u223cD\ndrewabunchof (x,y) pairsindependentlyatrandomfrom D,whatwouldyouraveragelossbe?More\nformally,if D isadiscreteprobabilitydistribution,thenthisexpectationcanbeexpandedas:\nE [(cid:96)(y, f(x))] = \u2211 [D(x,y)(cid:96)(y, f(x))] (1.1)\n(x,y)\u223cD\n(x,y)\u2208D\nThisisexactlytheweightedaveragelossovertheall (x,y) pairsin D,weightedbytheirprobability,\nD(x,y). If D isafinitediscretedistribution,forinstancedefinedbyafinitedataset {(x ,y ),...,(x ,y )\n1 1 N N\nthatputsequalweightoneachexample(probability1/N),thenweget:\nE [(cid:96)(y, f(x))] = \u2211 [D(x,y)(cid:96)(y, f(x))] definitionofexpectation (1.2)\n(x,y)\u223cD\n(x,y)\u2208D\nN\n= \u2211 [D(x n,y n)(cid:96)(y n, f(x n))] Disdiscreteandfinite (1.3)\nn=1\n=\n\u2211N\n[\n1\n(cid:96)(y n, f(x n))] definitionofD (1.4)\nN\nn=1\n=\n1 \u2211N\n[(cid:96)(y n, f(x n))] rearrangingterms (1.5)\nN\nn=1\nWhichisexactlytheaveragelossonthatdataset.\nThemostimportantthingtorememberisthattherearetwoequivalentwaystothinkaboutexpections:\n(1)Theexpectationofsomefunction g istheweightedaveragevalueof g,wheretheweightsaregivenby\ntheunderlyingprobabilitydistribution. (2)Theexpectationofsomefunction g isyourbestguessofthe\nvalueof g ifyouweretodrawasingleitemfromtheunderlyingprobabilitydistribution.\nFigure1.4:\nwilluseistheprobabilisticmodeloflearning. Namely,thereisaprob-\nabilitydistribution D overinput/outputpairs. Thisisoftencalled\nthedatageneratingdistribution. Ifwewrite x fortheinput(the\nuser/coursepair)and y fortheoutput(therating),then D isadistri-\nbutionover (x,y) pairs.\nAusefulwaytothinkabout D isthatitgiveshighprobabilityto\nreasonable (x,y) pairs,andlowprobabilitytounreasonable (x,y)\npairs. A (x,y) paircanbeunreasonableintwoways. First, x might\nbeanunusualinput. Forexample,a x relatedtoan\u201cIntrotoJava\u201d\ncoursemightbehighlyprobable;a x relatedtoa\u201cGeometricand\nSolidModeling\u201dcoursemightbelessprobable. Second, y might\nbeanunusualratingforthepaired x. Forinstance,ifAlicewereto\ntakeAI100times(withoutrememberingthatshetookitbefore!),\nshewouldgivethecoursea +2almosteverytime. Perhapssome\n16 a course in machine learning\nsemestersshemightgiveaslightlylowerscore,butitwouldbeun-\nlikelytosee x =Alice/AIpairedwith y = \u22122.\nItisimportanttorememberthatwearenotmakinganyassump-\ntionsaboutwhatthedistribution D lookslike. (Forinstance,we\u2019re\nnotassumingitlookslikeaGaussianorsomeother,commondistri-\nbution.) Wearealsonotassumingthatweknowwhat D is. Infact,\nifyouknowaprioriwhatyourdatageneratingdistributionis,your\nlearningproblembecomessignificantlyeasier. Perhapsthehardest\nthingaboutmachinelearningisthatwedon\u2019tknowwhat D is: allwe\ngetisarandomsamplefromit. Thisrandomsampleisourtraining\ndata.\nOurlearningproblem,then,isdefinedbytwoquantities:\nConsiderthefollowingprediction\ntask.Givenaparagraphwritten\n1. Thelossfunction (cid:96),whichcapturesournotionofwhatisimportant aboutacourse,wehavetopredict\nwhethertheparagraphisapositive\ntolearn.\n? ornegativereviewofthecourse.\n2. Thedatageneratingdistribution D,whichdefineswhatsortof (Thisisthesentimentanalysisprob-\nlem.)Whatisareasonableloss\ndataweexpecttosee. function?Howwouldyoudefine\nthedatageneratingdistribution?\nWearegivenaccesstotrainingdata,whichisarandomsampleof\ninput/outputpairsdrawnfrom D. Basedonthistrainingdata,we\nneedtoinduceafunction f thatmapsnewinputs x\u02c6 tocorresponding\nprediction y\u02c6. Thekeypropertythat f shouldobeyisthatitshoulddo\nwell(asmeasuredby (cid:96))onfutureexamplesthatarealsodrawnfrom\nD. Formally,it\u2019sexpectedloss (cid:101) over D withrepsectto (cid:96) shouldbe\nassmallaspossible:\n(cid:101)(cid:44)E (cid:2) (cid:96)(y, f(x))(cid:3) = \u2211 D(x,y)(cid:96)(y, f(x)) (1.6)\n(x,y)\u223cD\n(x,y)\nThedifficultyinminimizingourexpectedlossfromEq(1.6)is\nthatwedon\u2019tknowwhatD is! Allwehaveaccesstoissometraining\ndatasampledfromit! Supposethatwedenoteourtrainingdata\nsetby D. Thetrainingdataconsistsof N-manyinput/outputpairs,\n(x ,y ),(x ,y ),...,(x ,y ). Givenalearnedfunction f,wecan\n1 1 2 2 N N\ncomputeourtrainingerror, (cid:101)\u02c6:\n(cid:101)\u02c6 (cid:44)\n1 \u2211N\n(cid:96)(y , f(x )) (1.7)\nn n\nN\nn=1\nThatis,ourtrainingerrorissimplyouraverageerroroverthetrain-\ningdata.\nVerifybycalculationthatwe\nOfcourse,wecandrive (cid:101)\u02c6 tozerobysimplymemorizingourtrain- canwriteourtrainingerroras\ningdata. ButasAlicemightfindinmemorizingpastexams,this\nE (x,y)\u223cD(cid:2)(cid:96)(y,f(x))(cid:3) ,bythinking\n? ofDasadistributionthatplaces\nmightnotgeneralizewelltoanewexam! probability1/Ntoeachexamplein\nThisisthefundamentaldifficultyinmachinelearning: thething Dandprobability0oneverything\nelse.\nwehaveaccesstoisourtrainingerror, (cid:101)\u02c6. Butthethingwecareabout\ndecision trees 17\nminimizingisourexpectederror (cid:101). Inordertogettheexpectederror\ndown,ourlearnedfunctionneedstogeneralizebeyondthetraining\ndatatosomefuturedatathatitmightnothaveseenyet!\nSo,puttingitalltogether,wegetaformaldefinitionofinduction\nmachinelearning: Given(i)alossfunction (cid:96) and(ii)asample D\nfromsomeunknowndistribution D,youmustcomputeafunction\nf thathaslowexpectederror (cid:101) over D withrespectto (cid:96).\nAveryimportantcommentisthatweshouldneverexpectama-\nchinelearningalgorithmtogeneralizebeyondthedatadistribution\nithasseenattrainingtime. Inafamous\u2014ifposssiblyapocryphal\u2014\nexamplefromthe1970s,theUSGovernmentwantedtotrainaclas-\nsifiertodistinguishbetweenUStanksandRussiantanks. Theycol-\nlectedatrainingandtestset,andmanagedtobuildaclassifierwith\nnearly100%accuracyonthatdata. Butwhenthisclassifierwasrun\ninthe\u201crealworld\u201d,itfailedmiserably. Ithadnot,infact,learned\ntodistinguishbetweenUStanksandRussiantanks,butratherjust\nbetweenclearphotosandblurryphotos. Inthiscase,therewasabias\ninthetrainingdata(duetohowthetrainingdatawascollected)that\ncausedthelearningalgorithmtolearnsomethingotherthanwhatwe\nwerehopingfor. WewillreturntothisissueinChapter6;fornow,\nsimplyrememberthatthedistribution D fortrainingdatamustmatch\nthedistribution D forthetestdata.\n1.5 Chapter Summary and Outlook\nAtthispoint,youshouldbeabletousedecisiontreestodomachine\nlearning. Someonewillgiveyoudata. You\u2019llsplititintotraining,\ndevelopmentandtestportions. Usingthetraininganddevelopment\ndata,you\u2019llfindagoodvalueformaximumdepththattradesoff\nbetweenunderfittingandoverfitting. You\u2019llthenruntheresulting\ndecisiontreemodelonthetestdatatogetanestimateofhowwell\nyouarelikelytodointhefuture.\nYoumightthink: whyshouldIreadtherestofthisbook? Aside\nfromthefactthatmachinelearningisjustanawesomefunfieldto\nlearnabout,there\u2019salotlefttocover. Inthenexttwochapters,you\u2019ll\nlearnabouttwomodelsthathaveverydifferentinductivebiasesthan\ndecisiontrees. You\u2019llalsogettoseeaveryusefulwayofthinking\naboutlearning: thegeometricviewofdata. Thiswillguidemuchof\nwhatfollows. Afterthat,you\u2019lllearnhowtosolveproblemsmore\ncomplicatedthatsimplebinaryclassification. (Machinelearning\npeoplelikebinaryclassificationalotbecauseit\u2019soneofthesimplest\nnon-trivialproblemsthatwecanworkon.) Afterthat,thingswill\ndiverge: you\u2019lllearnaboutwaystothinkaboutlearningasaformal\noptimizationproblem,waystospeeduplearning,waystolearn\n18 a course in machine learning\nwithoutlabeleddata(orwithverylittlelabeleddata)andallsortsof\notherfuntopics.\nButthroughout,wewillfocusontheviewofmachinelearning\nthatyou\u2019veseenhere. Youselectamodel(anditsassociatedinduc-\ntivebiases). Youusedatatofindparametersofthatmodelthatwork\nwellonthetrainingdata. Youusedevelopmentdatatoavoidunder-\nfittingandoverfitting. Andyouusetestdata(whichyou\u2019llneverlook\natortouch,right?) toestimatefuturemodelperformance. Thenyou\nconquertheworld.\n1.6 Further Reading\nInourdiscussionofdecisiontrees,weusedmisclassificationratefor\nselectingfeatures. Whilesimpleandintuitive,misclassificationrate\nhasproblems. Therehasbeenasignificantamountofworkthat\nconsidersmoreadvancedsplittingcriteria;themostpopularisID3,\nbasedonthemutualinformationquantityfrominformationthe-\nory. Wehavealsoonlyconsideredaverysimplemechanismfor\ncontrollinginductivebias: limitingthedepthofthedecisiontree.\nAgain,therearemoreadvanced\u201ctreepruning\u201dtechniquesthattyp-\nicallyoperatebygrowingdeeptreesandthenpruningbacksome\nofthebranches. Theseapproacheshavetheadvantagethatdiffer-\nentbranchescanhavedifferentdepths,accountingforthefactthat\ntheamountofdatathatgetspasseddowneachbranchmightdiffer\n3\ndramatically . 3Quinlan1986\n",
    "explanation": "To understand the application of decision trees in real-world scenarios, particularly in healthcare, it's essential to grasp the fundamental concept of generalization. Decision trees are a type of machine learning model that helps in making predictions based on past data. Unlike memorization, which involves recalling exact past experiences, decision trees aim to generalize from the data they are trained on to make informed predictions about new, unseen data. This ability to generalize is crucial, especially in fields like healthcare, where predicting patient outcomes based on historical data can significantly impact treatment plans and patient care.\n\nLet's delve into a detailed example in healthcare to illustrate how decision trees work. Imagine a hospital wants to predict whether a patient is at risk of developing a certain condition based on their medical history and current health indicators. The decision tree model would be trained on a dataset containing various patient records, each with features such as age, blood pressure, cholesterol levels, and previous medical conditions. The model learns to ask a series of yes/no questions about these features to classify patients into risk categories. For instance, the tree might first ask if the patient's age is above a certain threshold, then whether their blood pressure is high, and so on, until it reaches a conclusion about the patient's risk level.\n\nA step-by-step walkthrough of this process involves several stages. Initially, the decision tree algorithm evaluates all available features to determine which one best splits the data into distinct categories. This is often done using measures like information gain, which quantifies how well a feature separates the data into the desired classes. The tree then recursively splits the data based on the most informative features until it reaches a stopping criterion, such as a maximum tree depth or a minimum number of samples in a leaf node. This process ensures that the model is not just memorizing the training data but is learning patterns that can be applied to new data.\n\nAddressing common misconceptions, it's important to note that decision trees do not simply memorize past data. Instead, they create a model that can generalize to new situations by identifying patterns and relationships within the data. This is why decision trees are powerful tools in healthcare, where they can help predict patient outcomes, diagnose diseases, and recommend treatments based on a wide range of patient data.\n\nWhen considering the technical and ethical aspects of using decision trees in healthcare, several factors come into play. Technically, it's crucial to ensure that the model is trained on a representative dataset to avoid biases that could lead to inaccurate predictions. Ethically, patient privacy must be safeguarded, and the model's predictions should be used to support, not replace, clinical judgment. Transparency in how the model makes decisions is also vital to maintain trust and accountability in healthcare settings.\n\nIn conclusion, decision trees are a versatile and powerful tool in machine learning, particularly in healthcare applications. They provide a structured way to make predictions based on historical data, emphasizing generalization over memorization. By understanding the process of building and using decision trees, we can harness their potential to improve decision-making in various domains, ensuring that the models are both technically sound and ethically responsible.",
    "short_term_trajectory": [
        "\n    Question: What is the content of the provided explanation?\n\nA) Detailed analysis  \nB) Summary  \nC) None  \nD) Comprehensive overview \n    Student Answer: C) None",
        "\n    Question: What is the primary advantage of using decision trees in healthcare applications?\n\nA) They memorize past patient data for accurate predictions.  \nB) They generalize from historical data to predict new patient outcomes.  \nC) They require no prior data for making predictions.  \nD) They replace clinical judgment with automated decisions. \n    Student Answer: A) They memorize past patient data for accurate predictions.",
        "\n    Question: What is a key factor in ensuring decision trees make accurate predictions in healthcare?\n\nA) Using a small, specific dataset to train the model.  \nB) Ensuring the model memorizes past patient data.  \nC) Training the model on a representative dataset to avoid biases.  \nD) Allowing the model to replace clinical judgment entirely. \n    Student Answer: B) Ensuring the model memorizes past patient data.",
        "\n    Question: What is the primary function of decision trees in machine learning, particularly in healthcare?\n\nA) To memorize exact past experiences for future predictions.  \nB) To generalize from training data to make predictions about new data.  \nC) To replace healthcare professionals in decision-making processes.  \nD) To ensure patient privacy by not using any historical data. \n    Student Answer: C) To replace healthcare professionals in decision-making processes.",
        "\n    Question: What is a crucial step in the process of building a decision tree model for healthcare applications?\n\nA) Memorizing all patient data for future reference.  \nB) Evaluating features to determine which best splits the data into categories.  \nC) Ensuring the model replaces clinical judgment.  \nD) Using a single feature to make all predictions. \n    Student Answer: Based on the explanation provided, the crucial step in the process of building a decision tree model for healthcare applications is:\n\nB) Evaluating features to determine which best splits the data into categories.\n\nThis step involves analyzing the available features to find the ones that most effectively separate the data into distinct categories, which is essential for the decision tree to make accurate predictions."
    ],
    "attempt": "What is a crucial step in the process of building a decision tree model for healthcare applications?\n\nA) Memorizing all patient data for future reference.  \nB) Evaluating features to determine which best splits the data into categories.  \nC) Ensuring the model replaces clinical judgment.  \nD) Using a single feature to make all predictions.",
    "student_answers": [
        "C) None",
        "A) They memorize past patient data for accurate predictions.",
        "B) Ensuring the model memorizes past patient data.",
        "C) To replace healthcare professionals in decision-making processes.",
        "Based on the explanation provided, the crucial step in the process of building a decision tree model for healthcare applications is:\n\nB) Evaluating features to determine which best splits the data into categories.\n\nThis step involves analyzing the available features to find the ones that most effectively separate the data into distinct categories, which is essential for the decision tree to make accurate predictions."
    ],
    "eval_result": "FAIL",
    "reflections": [
        "The student's responses indicate a misunderstanding of key concepts related to decision trees, particularly in the context of their application in healthcare. Let's analyze the potential reasons for this and suggest improvements to the explanation.\n\n### Potential Gaps or Shortcomings in the Explanation:\n\n1. **Complexity and Jargon**: The explanation might have been too technical or used jargon without sufficient explanation. Terms like \"information gain,\" \"Gini impurity,\" and \"pruning\" might not have been clearly defined or contextualized for someone new to the topic.\n\n2. **Overemphasis on Memorization**: The student seems to have misunderstood the role of memorization in decision trees, as evidenced by their incorrect answers. The explanation should have emphasized that decision trees aim to generalize from data rather than memorize it.\n\n3. **Lack of Emphasis on Generalization**: The explanation might not have sufficiently highlighted the importance of generalization in decision trees, leading to the student's misconception that decision trees primarily memorize past data.\n\n4. **Insufficient Examples**: While the healthcare example was provided, it might not have been detailed enough to illustrate how decision trees work in practice. More concrete examples or visual aids could help clarify the process.\n\n5. **Engagement and Interactivity**: The explanation might have been too one-sided, lacking interactive elements or questions to engage the student actively in the learning process.\n\n### Suggestions for Improvement:\n\n1. **Simplify and Define Terms**: Break down complex terms and concepts into simpler language. Provide definitions and examples for terms like \"information gain\" and \"pruning\" to ensure understanding.\n\n2. **Emphasize Generalization**: Clearly explain that decision trees are designed to generalize from training data to make predictions on unseen data. Highlight the difference between memorization and generalization.\n\n3. **Use Visual Aids**: Incorporate diagrams or flowcharts to visually represent how decision trees split data and make decisions. Visual aids can make abstract concepts more tangible.\n\n4. **Provide More Examples**: Use multiple examples from different domains, not just healthcare, to show the versatility of decision trees. This can help students see the broader applicability of the concept.\n\n5. **Interactive Elements**: Include questions or small exercises throughout the explanation to engage the student and check their understanding. This could involve asking them to predict the next step in a decision tree or identify features that might be important in a given scenario.\n\n6. **Feedback and Reflection**: Encourage students to reflect on their answers and provide feedback on why certain choices are correct or incorrect. This can help reinforce learning and correct misconceptions.\n\nBy addressing these areas, the explanation can be made more accessible, engaging, and effective in conveying the key concepts of decision trees and their applications."
    ],
    "passed": true,
    "trial": 1,
    "student": {
        "age": 50,
        "gender": "Female",
        "race": "Black",
        "education_level": "High School",
        "education_subject": "English",
        "occupation": "Housewife",
        "topics_learnt": [
            "Understanding Decision Trees Through Real-World Examples"
        ],
        "issue_topic": [
            "Introduction to Real-World Applications",
            "Detailed Example - Healthcare",
            "Step-by-Step Walkthrough",
            "Addressing Misconceptions",
            "Technical and Ethical Considerations",
            "Conclusion and Key Takeaways"
        ]
    }
}